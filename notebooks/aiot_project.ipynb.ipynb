{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIoT Project\n",
    "import os\n",
    "\n",
    "# basic data engineering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# db\n",
    "import pymongo\n",
    "\n",
    "# configs & other\n",
    "import yaml\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "from psynlig import pca_explained_variance_bar\n",
    "\n",
    "# utils processing\n",
    "from utils import sliding_window_pd\n",
    "from utils import apply_filter\n",
    "from utils import filter_instances\n",
    "from utils import flatten_instances_df\n",
    "from utils import df_rebase\n",
    "from utils import rename_df_column_values\n",
    "\n",
    "# utils visualization\n",
    "from utils_visual import plot_instance_time_domain\n",
    "from utils_visual import plot_instance_3d\n",
    "from utils_visual import plot_np_instance\n",
    "from utils_visual import plot_heatmap\n",
    "from utils_visual import plot_scatter_pca\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Start time of execution\n",
    "time_start = time()\n",
    "\n",
    "# Load configuration\n",
    "config_path = os.path.join(os.getcwd(), \"config.yml\")\n",
    "\n",
    "with open(config_path) as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "client = pymongo.MongoClient(config[\"client\"])\n",
    "db = client[config[\"db\"]]\n",
    "coll = db[config[\"col\"]]\n",
    "found_keys = coll.distinct(\"label\")\n",
    "print(\"Existing DB keys:\", found_keys)\n",
    "\n",
    "# Apply filter\n",
    "data_cursor = coll.find()\n",
    "data_list = list(data_cursor)\n",
    "data_df_list = [pd.DataFrame(doc['data']) for doc in data_list]\n",
    "\n",
    "filtered_data = filter_instances(data_df_list, order=config['filter']['order'], wn=config['filter']['wn'], filter_type=config['filter']['type'])\n",
    "\n",
    "# Transform the list of DataFrames to NumPy array\n",
    "np_data_list = [df.to_numpy() for df in filtered_data]\n",
    "\n",
    "# Applying sliding window\n",
    "windows_list = [sliding_window_pd(pd.DataFrame(data), ws=config['sliding_window']['ws'], overlap=config['sliding_window']['overlap']) for data in np_data_list]\n",
    "\n",
    "# Flatten the 2D window instances\n",
    "windows_flattened = [flatten_instances_df(windows) for windows in windows_list]\n",
    "\n",
    "# Combine all windows into a single DataFrame\n",
    "all_windows_flattened = pd.concat(windows_flattened, ignore_index=True)\n",
    "\n",
    "# Extract features and labels\n",
    "X = all_windows_flattened.iloc[:, :-1].values\n",
    "y = all_windows_flattened.iloc[:, -1].values\n",
    "\n",
    "# Train/Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=True, random_state=42)\n",
    "\n",
    "# Scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Dimensionality Reduction with PCA using the 1D (flattened) data\n",
    "# add transformers\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA with 2 Components\n",
    "pca2d = PCA(n_components=2)\n",
    "X_train_2d = pca2d.fit_transform(X_train_scaled)\n",
    "X_test_2d = pca2d.transform(X_test_scaled)\n",
    "\n",
    "pca_explained_variance_bar(pca2d, alpha=0.8)\n",
    "plot_scatter_pca(X_train_2d, y_train, title='PCA 2D Scatter Plot')\n",
    "\n",
    "# PCA with 3 Components\n",
    "pca3d = PCA(n_components=3)\n",
    "X_train_3d = pca3d.fit_transform(X_train_scaled)\n",
    "X_test_3d = pca3d.transform(X_test_scaled)\n",
    "plot_instance_3d(X_train_3d, y_train)\n",
    "\n",
    "# PCA with X% of the variance of the dataset, for training the statistical AI Models\n",
    "pca_var = PCA(n_components=0.95)\n",
    "X_train_var = pca_var.fit_transform(X_train_scaled)\n",
    "X_test_var = pca_var.transform(X_test_scaled)\n",
    "\n",
    "# Classifier - Statistical Learning\n",
    "# Apply simple classifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "svc = SVC(kernel='rbf', C=config['classifier']['SVC']['C'], gamma=config['classifier']['SVC']['gamma'])\n",
    "svc.fit(X_train_var, y_train)\n",
    "\n",
    "# Evaluate simple classifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "y_pred = svc.predict(X_test_var)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Apply optimization with Grid Search and Cross-validation\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = config['fine_tune']['param_grid']\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=SVC(), param_grid=param_grid, cv=config['fine_tune']['cv'], verbose=config['fine_tune']['verbose'])\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train_var, y_train)\n",
    "\n",
    "# Evaluate optimized classifier\n",
    "best_svc = grid_search.best_estimator_\n",
    "y_pred_optimized = best_svc.predict(X_test_var)\n",
    "cm_optimized = confusion_matrix(y_test, y_pred_optimized)\n",
    "disp_optimized = ConfusionMatrixDisplay(confusion_matrix=cm_optimized)\n",
    "disp_optimized.plot()\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, y_pred_optimized))\n",
    "\n",
    "# Classifier - Neural Network\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Flatten\n",
    "\n",
    "# Create the Neural Network (NN) Architecture and instantiate the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers to the model\n",
    "model.add(Dense(64, activation='relu', input_shape=X_train_2d.shape[1:]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(len(np.unique(y)), activation='softmax'))\n",
    "\n",
    "# Plot the Architecture of the TensorFlow model\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "\n",
    "# Plot the summary of the TensorFlow model\n",
    "model.summary()\n",
    "\n",
    "# Build the NN model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Encode labels\n",
    "from utils import encode_labels\n",
    "y_train_encoded = encode_labels(y_train)\n",
    "y_test_encoded = encode_labels(y_test)\n",
    "\n",
    "# Train the NN model\n",
    "history = model.fit(X_train_2d, y_train_encoded, epochs=config['fit']['epochs'], batch_size=config['fit']['batch'], validation_split=0.2, verbose=config['fit']['verbose'])\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(X_test_2d, y_test_encoded)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Plot and interpret the learning curves: Loss and Accuracy based on the training and validation sets\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# End time of execution\n",
    "time_end = time()\n",
    "print(f\"Total execution time: {time_end - time_start:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
